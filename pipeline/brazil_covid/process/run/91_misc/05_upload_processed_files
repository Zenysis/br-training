#!/bin/bash -eu
# The platform offers direct downloads for processed data.
set -o pipefail

source "${PIPELINE_UTILS_DIR}/bash/common.sh"

UPLOAD_FILE="${PIPELINE_TMP_DIR}/epi_data.csv"
REMOTE_DESTINATION="s3/zenysis-br-covid-assets/public_downloads"

STEP='upload_processed_files'
SOURCES=($("${PIPELINE_SRC_ROOT}/data/pipeline/scripts/generate_pipeline_sources.py" ${STEP}))

pids=()
for cur_source in "${SOURCES[@]}" ; do
  source_out_dir="${PIPELINE_OUT_ROOT}/out/${cur_source}/${PIPELINE_DATE}"

  # Put it all in one file...
  combined_file="${PIPELINE_TMP_DIR}/${cur_source}.json.gz"
  cat "${source_out_dir}/processed_rows"* > ${combined_file}

  # Upload it
  UploadFile "${combined_file}" "${REMOTE_DESTINATION}" &

  pids+=("$!")
done

# Wait on each background process individually so that non-zero exit codes
# will be raised
WaitMultipleThreads "${pids[@]}"
